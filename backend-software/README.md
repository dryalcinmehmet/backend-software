#Mehmet Yalcin

#Home Project

#Notes: 
System can works on docker machine as a swarm mode. You can easly deploy with applying instructions.

#creating manager and workers

    docker-machine create --driver virtualbox manager

    docker-machine create --driver virtualbox worker1

    docker-machine create --driver virtualbox worker2

    docker-machine create --driver virtualbox worker3

#connect manager and start swarm
     
    eval $(docker-machine env manager)

#give command
     
    docker swarm init --advertise-addr eth1

#copy code like below that swarm has given 
     
    docker swarm join --token SWMTKN-1-46rhtokdqdi9r64stiieoeek4gbmpvozn9snv93kfl14cj1air-4t525v014h0ot6ubs88mjkv9z 192.168.99.152:2377

#connect worker machines and define them as a worker node. give commands.
  
      eval $(docker-machine env worker1)

      docker swarm join --token SWMTKN-1-46rhtokdqdi9r64stiieoeek4gbmpvozn9snv93kfl14cj1air-4t525v014h0ot6ubs88mjkv9z 192.168.99.152:2377


      eval $(docker-machine env worker2)
  
      docker swarm join --token SWMTKN-1-46rhtokdqdi9r64stiieoeek4gbmpvozn9snv93kfl14cj1air-4t525v014h0ot6ubs88mjkv9z 192.168.99.152:2377

      eval $(docker-machine env worker3)
  
      docker swarm join --token SWMTKN-1-46rhtokdqdi9r64stiieoeek4gbmpvozn9snv93kfl14cj1air-4t525v014h0ot6ubs88mjkv9z 192.168.99.152:2377

Note: This join tokens just an example. You should copy your token code first.

      eval $(docker-machine env manager)

      docker service create --name registry --publish published=5000,target=5000 registry:2

      cd backend-software
    
      docker build -t backend-software .
    
      docker-compose -f docker-compose.yml build
    
      docker-compose -f docker-compose.yml push
      
      docker stack deploy --compose-file docker-compose.yml backend-software

note: if ".docker/machine/machines/manager/ca.pem: no such file or directory" error exist use command
            
       eval $(docker-machine env -u)

#Manual Run

just give command:
  
    pip install -r requirements.txt
   
    python manage.py migrate
   
    python manage.py createsuperuser
   
    python manage.py runserver

 - You can manage models in admin.

#API Endpoints
- 127.0.0.1:8000/api/v1/login --> payload = {"username":admin,"password":"-"}
  
Get Token from login and add endpoints that been below

Header --> Authorization Token token_number

- 127.0.0.1:8000/api/v1/dsrs
  
- 127.0.0.1:8000/api/v1/dsrs

- 127.0.0.1:8000/api/v1/dsrs/1

- http://127.0.0.1:8007/api/v1/resources/percentile/:number?territory=non aliquip quis id&period_start=2021-07-17&period_end=2022-07-17


# Digital - Senior Engineer test


A lot of our work is about connecting digital service providers (DSPs) like 
Spotify or YouTube with societies like SGAE or SACEM, who represent music 
creators. DSPs provide digital sales reports (DSRs), which contain information 
about music metadata and revenue generated. We crunch this data and give 
societies the information they need.

For this test, we provide several DSRs that represent the usages and revenue 
from different countries. The aim is to parse the contents of the DSRs and 
insert them into a database to extract statistics through an API. Each line of 
a DSR represents a sound recording and its associated usage data. In detail, 
it contains the following fields:

    dsp_id: the unique identifier of a sound recording provided by DSP.
    title: sound recording title.
    artists: pipe-separated list of artists.
    isrc: International Sound Recording Code.
    usages: number of plays for this sound recording, territory and period.
    revenue: revenue generated by this sound recording, territory and period.

DSR filenames specify metadata related to the DSR, such as Territory, Period, 
and Currency. You will find the DSRs in the `data/` directory.

The API specification is provided as an OpenAPI specification:

    openapi.md

Our current (and incomplete) database contains the following tables:

* DSR: Models the DSR file and stores some relevant information.
* Currency: Models a currency.
* Territory: Models a territory.

Deliverables:

* A way to import the contents of DSRs to the DB.
* Complete the API according to the OpenAPI specification.
* A form in the admin page to delete DSRs and it's contents.
* Tests for each api endpoint, using any preferred testing framework.
* Dockerfile

Requirements:

* Django 3.1
* Python 3.9

Extra questions:

* DSPs report DSRs containing hundreds of millions of usages. If you were to 
  deploy this solution to production, would you do any change in the database 
  or process, in order to import the usages? Which ones?

Note:

In order to manage python dependencies, it will be necessary to use any tool 
(e.g.: pipenv) that interprets the Pipfile placed in the root folder.

For example, using `pipenv`, it's enough to do:

    pipenv sync --dev

